import chainlit as cl
import pymupdf4llm
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from langchain.schema.runnable import Runnable
from langchain.schema.runnable.config import RunnableConfig
from typing import cast
from time import sleep
from tavily import TavilyClient
from dotenv import load_dotenv
import json
import os

# todo : from langchain_community.document_loaders import TavilyLoader
# load .env file to environment
load_dotenv()

OPEN_API_KEY = os.getenv("OPENAI_API_KEY")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
TAVILY_API_KEY = os.getenv("TAVILY_KEY")
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)


def get_course_recommendations(jobs_context):
    search_query = f"Coursera specializations related to {jobs_context[0]} and {jobs_context[1]} . Only the links related to actual specializations"

    # Perform Tavily web search
    search_results = tavily_client.search(
        query=search_query,
        num_results=5,
        search_depth="advanced",
        include_answer="basic",
        include_domains=["COURSERA.ORG"],
    )

    # Extract course titles, links, and descriptions
    course_recommendations = []
    for result in search_results.get("results", []):
        if result.get("url").startswith(
            "https://www.coursera.org/specializations"
        ) or result.get("url").startswith(
            "https://www.coursera.org/professional-certificates"
        ):
            course_recommendations.append(
                {
                    "title": result.get("title"),
                    "url": result.get("url"),
                    "description": result.get("content"),
                }
            )

    return course_recommendations


@cl.on_message
async def main(message: cl.Message):
    await cl.Message(content=f"Received : {message.content}").send()


@cl.on_chat_start
async def on_chat_start():
    msg = cl.Message(
        content="Welcome to Tech Career Counselor. Let me setup the system"
    )
    await msg.send()

    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
    embedding_dim = 1536
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
    RESUME_COLLECTION_NAME = "resume_embeddings"
    PERSONAS_COLLECTION_NAME = "personas"
    JOBS_COLLECTION_NAME = "job_definitions"

    client.recreate_collection(
        collection_name=RESUME_COLLECTION_NAME,
        vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE),
    )

    print("‚úÖ Collection setup complete!")
    sleep(4)

    msg.content = "The system is ready. Let me ask you a few questions to understand what you're looking for!"
    await msg.update()

    model = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are a kind career counselor for tech roles who provides help for career advice. You are here to help the user find the best career path for them.
                Use the following context to answer accurately:\n\n{retrieved_context}""",
            ),
            ("human", "{question}"),
        ]
    )
    runnable = prompt | model | StrOutputParser()
    cl.user_session.set("runnable", runnable)

    files = None

    ####################################################################################################

    # 1Ô∏è‚É£ Mode de travail pr√©f√©r√© ?
    work_mode = await cl.AskActionMessage(
        content="Quel est votre mode de travail pr√©f√©r√© ?",
        actions=[
            cl.Action(
                name="remote",
                payload={"value": "remote"},
                label="√Ä distance (full remote)",
            ),
            cl.Action(name="hybrid", payload={"value": "hybrid"}, label="Hybride"),
            cl.Action(
                name="onsite", payload={"value": "onsite"}, label="Sur site uniquement"
            ),
        ],
    ).send()

    if work_mode:
        work_mode_value = work_mode.get("payload", {}).get("value")

    # 2Ô∏è‚É£ Pr√©f√©rez-vous travailler seul ou en √©quipe ?
    teamwork_preference = await cl.AskActionMessage(
        content="Pr√©f√©rez-vous travailler seul ou en √©quipe ?",
        actions=[
            cl.Action(name="alone", payload={"value": "alone"}, label="Plut√¥t seul"),
            cl.Action(name="team", payload={"value": "team"}, label="Plut√¥t en √©quipe"),
            cl.Action(name="both", payload={"value": "both"}, label="Un peu des deux"),
        ],
    ).send()

    if teamwork_preference:
        teamwork_value = teamwork_preference.get("payload", {}).get("value")

    # 3Ô∏è‚É£ Quel niveau d‚Äôautonomie souhaitez-vous ?
    autonomy_level = await cl.AskActionMessage(
        content="Quel niveau d‚Äôautonomie souhaitez-vous ?",
        actions=[
            cl.Action(
                name="independent",
                payload={"value": "independent"},
                label="Ind√©pendance totale",
            ),
            cl.Action(
                name="managed",
                payload={"value": "managed"},
                label="Encadr√© par un manager ou une √©quipe",
            ),
            cl.Action(
                name="balanced",
                payload={"value": "balanced"},
                label="√âquilibre entre autonomie et encadrement",
            ),
        ],
    ).send()

    if autonomy_level:
        autonomy_value = autonomy_level.get("payload", {}).get("value")

    # 4Ô∏è‚É£ Quel niveau de cr√©ativit√© souhaitez-vous ?
    creativity_level = await cl.AskActionMessage(
        content="Quel niveau de cr√©ativit√© souhaitez-vous dans votre travail ?",
        actions=[
            cl.Action(name="high", payload={"value": "high"}, label="Tr√®s cr√©atif"),
            cl.Action(
                name="medium",
                payload={"value": "medium"},
                label="Technique mais avec un peu de cr√©ativit√©",
            ),
            cl.Action(
                name="low",
                payload={"value": "low"},
                label="Pas forc√©ment cr√©atif, mais logique et analytique",
            ),
        ],
    ).send()

    if creativity_level:
        creativity_value = creativity_level.get("payload", {}).get("value")

    # 5Ô∏è‚É£ Quel type d‚Äôenvironnement de travail pr√©f√©rez-vous ?
    work_environment = await cl.AskActionMessage(
        content="Quel type d‚Äôenvironnement de travail pr√©f√©rez-vous ?",
        actions=[
            cl.Action(name="startup", payload={"value": "startup"}, label="Start-up"),
            cl.Action(
                name="corporate",
                payload={"value": "corporate"},
                label="Grande entreprise",
            ),
            cl.Action(
                name="freelance",
                payload={"value": "freelance"},
                label="Freelance / autoentrepreneur",
            ),
        ],
    ).send()

    if work_environment:
        work_environment_value = work_environment.get("payload", {}).get("value")

    # 6Ô∏è‚É£ Pr√©f√©rez-vous coder ou √©viter le code ?
    coding_preference = await cl.AskActionMessage(
        content="Pr√©f√©rez-vous coder ou √©viter le code ?",
        actions=[
            cl.Action(
                name="love_coding",
                payload={"value": "love_coding"},
                label="J‚Äôaime coder / je veux apprendre",
            ),
            cl.Action(
                name="some_code",
                payload={"value": "some_code"},
                label="Je veux un m√©tier avec peu de code",
            ),
            cl.Action(
                name="no_code",
                payload={"value": "no_code"},
                label="Je pr√©f√®re un m√©tier sans code",
            ),
        ],
    ).send()

    if coding_preference:
        coding_value = coding_preference.get("payload", {}).get("value")

    # 7Ô∏è‚É£ Quel type de probl√®mes aimez-vous r√©soudre ?
    problem_solving = await cl.AskActionMessage(
        content="Quel type de probl√®mes aimez-vous r√©soudre ?",
        actions=[
            cl.Action(
                name="technical",
                payload={"value": "technical"},
                label="Probl√®mes techniques",
            ),
            cl.Action(
                name="strategic",
                payload={"value": "strategic"},
                label="Probl√®mes strat√©giques",
            ),
            cl.Action(
                name="data", payload={"value": "data"}, label="Probl√®mes de donn√©es"
            ),
            cl.Action(
                name="human",
                payload={"value": "human"},
                label="Probl√®mes humains et organisationnels",
            ),
        ],
    ).send()

    if problem_solving:
        problem_value = problem_solving.get("payload", {}).get("value")

    # 8Ô∏è‚É£ Quel √©quilibre entre routine et nouveaut√© recherchez-vous ?
    routine_novelty = await cl.AskActionMessage(
        content="Quel √©quilibre entre routine et nouveaut√© recherchez-vous ?",
        actions=[
            cl.Action(
                name="stable",
                payload={"value": "stable"},
                label="J‚Äôaime la stabilit√© et les t√¢ches bien d√©finies",
            ),
            cl.Action(
                name="dynamic",
                payload={"value": "dynamic"},
                label="J‚Äôaime la nouveaut√© et l‚Äôadaptation constante",
            ),
            cl.Action(
                name="balanced",
                payload={"value": "balanced"},
                label="J‚Äôaime l‚Äô√©quilibre entre les deux",
            ),
        ],
    ).send()

    if routine_novelty:
        routine_novelty_value = routine_novelty.get("payload", {}).get("value")

    # ‚úÖ Store all collected responses
    user_answers = {
        "work_mode": work_mode_value,
        "teamwork_preference": teamwork_value,
        "autonomy_level": autonomy_value,
        "creativity_level": creativity_value,
        "work_environment": work_environment_value,
        "coding_preference": coding_value,
        "problem_solving": problem_value,
        "routine_novelty": routine_novelty_value,
    }

    cl.user_session.set("personality_test_results", user_answers)

    # üéØ Final message
    await cl.Message(
        content="‚úÖ Merci pour vos r√©ponses ! Nous analysons vos pr√©f√©rences."
    ).send()

    # üîµ Retrieve the stored Runnable from session storage
    runnable = cast(Runnable, cl.user_session.get("runnable"))

    ####################################################################################################
    ##Persona analysis
    ##################################################################################################
    persona_vector_store = QdrantVectorStore(
        client=client,
        collection_name=PERSONAS_COLLECTION_NAME,
        embedding=embedding_model,
    )
    persona_retriever = persona_vector_store.as_retriever(search_kwargs={"k": 2})
    persona_search_results = persona_retriever.invoke(
        f"regarding  {str(user_answers)}, tell the user about the  persona that suit him "
    )
    # TODO: USE SESSIONS
    persona_retrieved_context = "\n\n".join(
        [doc.page_content for doc in persona_search_results]
    )

    question = "From the context, explain the 2 personas that suits the user and why.Don't include the jobs related to the persona"
    answer_persona = cl.Message(content="")
    persona_generated_response = ""
    # Run the pipeline using the retrieved Runnable
    async for chunk in runnable.astream(
        {"question": question, "retrieved_context": persona_retrieved_context},
        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),
    ):
        await answer_persona.stream_token(chunk)
        persona_generated_response += chunk

    await answer_persona.send()

    ####################################################################################################
    ##Resume analysis
    ##################################################################################################

    await cl.Message(
        content="Let's know more about your previous experiences! You surely have some transferable skills."
    ).send()

    #     # Wait for the user to upload a file
    while files is None:
        files = await cl.AskFileMessage(
            content="Please upload a text file to begin!",
            accept=["application/pdf"],
            max_size_mb=4,
            # timeout=180,
        ).send()
    ####################################################################################################
    ##Resume analysis
    ##################################################################################################
    file = files[0]

    loader = PyMuPDFLoader(file.path)
    resume_embeddings = loader.load()

    resume_vector_db = QdrantVectorStore.from_documents(
        documents=resume_embeddings,
        embedding=embedding_model,
        collection_name=RESUME_COLLECTION_NAME,
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
    )
    resume_vector_store = QdrantVectorStore(
        client=client,
        collection_name=RESUME_COLLECTION_NAME,
        embedding=embedding_model,
    )
    # todo: USE LANGCHAIN RETRIEVER
    resume_retriever = resume_vector_store.as_retriever(search_kwargs={"k": 10})
    # TODO: USE SESSIONS
    resume_search_results = resume_retriever.invoke(
        "Make a summarization of the skills and experiences of this user"
    )

    resume_retrieved_context = "\n\n".join(
        [doc.page_content for doc in resume_search_results]
    )

    question = "From the context, what are the skills or experiences that can be used in a tech career?"
    answer_resume = cl.Message(content="")
    resume_generated_response = ""

    # Run the pipeline using the retrieved Runnable
    async for chunk in runnable.astream(
        {"question": question, "retrieved_context": resume_retrieved_context},
        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),
    ):
        await answer_resume.stream_token(chunk)
        resume_generated_response += chunk

    await answer_resume.send()
    ####################################################################################################
    ##Global analysis
    ##################################################################################################
    jobs_vector_store = QdrantVectorStore(
        client=client,
        collection_name=JOBS_COLLECTION_NAME,
        embedding=embedding_model,
    )
    jobs_retriever = jobs_vector_store.as_retriever(search_kwargs={"k": 2})
    jobs_search_results = jobs_retriever.invoke(
        f"tell the user about the job that suit him based on his resume transferable skills and his persona from this context {str(resume_generated_response) + ' ' + str(persona_generated_response)}"
    )

    jobs = []
    for job in jobs_search_results:
        jobs.append(json.loads(job.page_content)["job_name"])

    jobs_retrieved_context = "\n\n".join(
        [doc.page_content for doc in jobs_search_results]
    )

    question = "Explain the 2 jobs that suits the user and why"
    answer_jobs = cl.Message(content="")
    jobs_generated_response = ""
    # Run the pipeline using the retrieved Runnable
    async for chunk in runnable.astream(
        {
            "question": question,
            "retrieved_context": jobs_retrieved_context
            + " "
            + str(resume_generated_response)
            + " "
            + str(persona_generated_response),
        },
        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),
    ):
        await answer_jobs.stream_token(chunk)
        jobs_generated_response += chunk

    await answer_jobs.send()

    ####################################################################################################
    ##Course recommendation
    ##################################################################################################

    # Voudrais tu que je te recommande des formations li√©es √† ces carri√®res ?
    will_recommend = await cl.AskActionMessage(
        content="Voudriez-vous que je vous recommande des formations li√©es √† ces carri√®res ?",
        actions=[
            cl.Action(name="oui", payload={"value": "oui"}, label="Oui"),
            cl.Action(name="non", payload={"value": "non"}, label="Non"),
        ],
    ).send()

    if will_recommend:
        will_recommend_value = will_recommend.get("payload", {}).get("value")
        if will_recommend_value == "oui":
            course_recommendations = get_course_recommendations(jobs)
            if course_recommendations:
                await cl.Message(
                    content="Voici quelques sp√©cialisations Coursera qui pourraient vous int√©resser :"
                ).send()
                for course in course_recommendations:
                    await cl.Message(
                        content=f"üîó [{course['title']}]({course['url']})\n\n{course['description']}"
                    ).send()
            else:
                await cl.Message(
                    content="D√©sol√©, je n'ai pas trouv√© de sp√©cialisations Coursera correspondant √† vos int√©r√™ts."
                ).send()
        else:
            await cl.Message(content="D'accord, merci pour votre temps!").send()
